{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy hugging face to Triton Inference Server on AKS\n",
    "\n",
    "description: Deploy a bert model to AKS GPU cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that this Public Preview release is subject to the [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "subscription_id = os.getenv(\"SUBSCRIPTION_ID\", default=\"<subscription_id>\")\n",
    "resource_group = os.getenv(\"RESOURCE_GROUP\", default=\"<resource_group>\")\n",
    "workspace_name = os.getenv(\"WORKSPACE_NAME\", default=\"<workspace_name>\")\n",
    "\n",
    "ws = Workspace.get(\n",
    "    subscription_id = subscription_id, \n",
    "    resource_group = resource_group, \n",
    "    name = workspace_name)\n",
    "\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "model = Model(ws, id=\"bert-base:1\")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy webservice\n",
    "\n",
    "Deploy to a pre-created [AksCompute](https://docs.microsoft.com/python/api/azureml-core/azureml.core.compute.aks.akscompute?view=azure-ml-py#provisioning-configuration-agent-count-none--vm-size-none--ssl-cname-none--ssl-cert-pem-file-none--ssl-key-pem-file-none--location-none--vnet-resourcegroup-name-none--vnet-name-none--subnet-name-none--service-cidr-none--dns-service-ip-none--docker-bridge-cidr-none--cluster-purpose-none--load-balancer-type-none-) named `aks-gpu`. For other options, see [our documentation](https://docs.microsoft.com/azure/machine-learning/how-to-deploy-and-where?tabs=azcli).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AksWebservice\n",
    "from azureml.core.model import InferenceConfig\n",
    "from random import randint\n",
    "\n",
    "service_name = \"bert-ncd-aks-gpu\"\n",
    "\n",
    "config = AksWebservice.deploy_configuration(\n",
    "    compute_target_name=\"aks-gpu\",\n",
    "    gpu_cores=1,\n",
    "    cpu_cores=1,\n",
    "    memory_gb=8,\n",
    "    auth_enabled=True,\n",
    ")\n",
    "\n",
    "service = Model.deploy(\n",
    "    workspace=ws,\n",
    "    name=service_name,\n",
    "    models=[model],\n",
    "    deployment_config=config,\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "service.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(service.get_logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the webservice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_key = service.get_keys()[0]\n",
    "scoring_uri = service.scoring_uri\n",
    "uri = scoring_uri[7:]\n",
    "print(service_key)\n",
    "print(scoring_uri)\n",
    "print(uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!curl -v $scoring_uri/v2/health/ready -H 'Authorization: Bearer '\"$service_key\"''\n",
    "!curl -k -X POST -v $scoring_uri/v2/service/bert-ncd-aks-gpu/v2/repository/index -H 'Authorization: Bearer '\"$service_key\"''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tritonclient.http as tritonhttpclient\n",
    "\n",
    "headers = {}\n",
    "headers[\"Authorization\"] = f\"Bearer {service_key}\"\n",
    "\n",
    "triton_client = tritonhttpclient.InferenceServerClient(uri)\n",
    "\n",
    "model_name = \"bert-base-cased\"\n",
    "\n",
    "# Check the state of server.\n",
    "health_ctx = triton_client.is_server_ready(headers=headers)\n",
    "print(\"Is server ready - {}\".format(health_ctx))\n",
    "\n",
    "# Check the status of model.\n",
    "status_ctx = triton_client.is_model_ready(model_name, \"1\", headers)\n",
    "print(\"Is model ready - {}\".format(status_ctx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert.tokenization  import BertTokenizer\n",
    "from bert.preprocess import preprocess_tokenized_text\n",
    "from tritonclient.utils import triton_to_np_dtype\n",
    "\n",
    "context = \"Within the genitourinary and gastrointestinal tracts, commensal flora serve as biological barriers by competing with pathogenic bacteria for food and space and, in some cases, by changing the conditions in their environment, such as pH or available iron. This reduces the probability that pathogens will reach sufficient numbers to cause illness. However, since most antibiotics non-specifically target bacteria and do not affect fungi, oral antibiotics can lead to an overgrowth of fungi and cause conditions such as a vaginal candidiasis (a yeast infection). There is good evidence that re-introduction of probiotic flora, such as pure cultures of the lactobacilli normally found in unpasteurized yogurt, helps restore a healthy balance of microbial populations in intestinal infections in children and encouraging preliminary data in studies on bacterial gastroenteritis, inflammatory bowel diseases, urinary tract infection and post-surgical infections.\"\n",
    "query = \"Most antibiotics target bacteria and don't affect what class of organisms?\"\n",
    "\n",
    "tokenizer = BertTokenizer('bert/vocab.txt', max_len=512)\n",
    "\n",
    "query_tokens = tokenizer.tokenize(query)\n",
    "\n",
    "print(query_tokens)\n",
    "\n",
    "feature = preprocess_tokenized_text(context, query_tokens, tokenizer)\n",
    "\n",
    "tensors_for_inference, tokens_for_postprocessing = feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tritonclient.utils import triton_to_np_dtype\n",
    "import numpy as np\n",
    "\n",
    "model_metadata = triton_client.get_model_metadata(model_name=model_name, headers=headers)\n",
    "\n",
    "input_meta = model_metadata[\"inputs\"]\n",
    "output_meta = model_metadata[\"outputs\"]\n",
    "\n",
    "np_dtype = triton_to_np_dtype(input_meta[0][\"datatype\"])\n",
    "\n",
    "input_ids = np.array(tensors_for_inference.input_ids, dtype=np_dtype)[None,...] # make bs=1\n",
    "segment_ids = np.array(tensors_for_inference.segment_ids, dtype=np_dtype)[None,...] # make bs=1\n",
    "input_mask = np.array(tensors_for_inference.input_mask, dtype=np_dtype)[None,...] # make bs=1\n",
    "\n",
    "input_mapping = {\n",
    "    \"input_ids\": input_ids,\n",
    "    \"token_type_ids\": segment_ids,\n",
    "    \"attention_mask\": input_mask,\n",
    "}\n",
    "\n",
    "inputs = []\n",
    "outputs = []\n",
    "        \n",
    "# Populate the inputs array\n",
    "for in_meta in input_meta:\n",
    "    input_name = in_meta[\"name\"]\n",
    "    data = input_mapping[input_name]\n",
    "\n",
    "    input = tritonhttpclient.InferInput(input_name, data.shape, in_meta[\"datatype\"])\n",
    "    input.set_data_from_numpy(data, binary_data=False)\n",
    "    inputs.append(input)\n",
    "\n",
    "# Populate the outputs array\n",
    "for out_meta in output_meta:\n",
    "    output_name = out_meta[\"name\"]\n",
    "    output = tritonhttpclient.InferRequestedOutput(output_name, binary_data=False)\n",
    "    outputs.append(output)\n",
    "\n",
    "            \n",
    "# Run inference\n",
    "res = triton_client.infer(\n",
    "    model_name,\n",
    "    inputs,\n",
    "    request_id=\"0\",\n",
    "    outputs=outputs,\n",
    "    model_version=\"1\",\n",
    "    headers=headers,\n",
    ")\n",
    "\n",
    "for output in res._result['outputs']:\n",
    "    print(output['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert.postprocess import get_answer\n",
    "import json\n",
    "\n",
    "start_logits = res.as_numpy(\"output_0\")[0]\n",
    "end_logits = res.as_numpy(\"output_1\")[0]\n",
    "\n",
    "# post-processing\n",
    "doc_tokens = context.split()\n",
    "answer, answers = get_answer(doc_tokens, tokens_for_postprocessing, start_logits[1], end_logits)\n",
    "    \n",
    "# print result\n",
    "print(answer)\n",
    "print(answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the webservice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "service.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "\n",
    "Try reading [our documentation](https://aka.ms/triton-aml-docs) to use Triton with your own models or check out the other notebooks in this folder for ways to do pre- and post-processing on the server. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "name": "deploy-bidaf-aks",
  "task": "Use the high-performance Triton Inference Server with Azure Machine Learning"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
