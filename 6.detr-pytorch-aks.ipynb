{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Detectron2 resnet model to Triton Inference Server on AKS\n",
    "\n",
    "description: In this example a pretrained resnet model from TorchVision is exported to TorchScript format. This exported model is then deployed to Triton on AKS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that this Public Preview release is subject to the [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "subscription_id = os.getenv(\"SUBSCRIPTION_ID\", default=\"<subscription_id>\")\n",
    "resource_group = os.getenv(\"RESOURCE_GROUP\", default=\"<resource_group>\")\n",
    "workspace_name = os.getenv(\"WORKSPACE_NAME\", default=\"<workspace_name>\")\n",
    "\n",
    "ws = Workspace.get(\n",
    "    subscription_id = subscription_id, \n",
    "    resource_group = resource_group, \n",
    "    name = workspace_name)\n",
    "\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download pretrained model\n",
    "Download the pretrained resnet model from TorchVision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "r18 = models.resnet18(pretrained=True)       # We now have an instance of the pretrained model\n",
    "r18_scripted = torch.jit.script(r18)         # *** This is the TorchScript export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "dummy_input = torch.rand(1, 3, 224, 224)     # We should run a quick test\n",
    "unscripted_output = r18(dummy_input)         # Get the unscripted model's prediction...\n",
    "scripted_output = r18_scripted(dummy_input)  # ...and do the same for the scripted version\n",
    "\n",
    "unscripted_top5 = F.softmax(unscripted_output, dim=1).topk(5).indices\n",
    "scripted_top5 = F.softmax(scripted_output, dim=1).topk(5).indices\n",
    "\n",
    "print('Python model top 5 results:\\n  {}'.format(unscripted_top5))\n",
    "print('TorchScript model top 5 results:\\n  {}'.format(scripted_top5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the exported model\n",
    "r18_scripted.save('models/triton/resnet18/1/model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from azureml.core.model import Model\n",
    "\n",
    "prefix = Path(\".\")\n",
    "model_path = prefix.joinpath(\"models\")\n",
    "\n",
    "model = Model.register(\n",
    "    model_path=model_path,\n",
    "    model_name=\"detectron2\",\n",
    "    tags={\"area\": \"image detection\", \"type\": \"detectron2 detr\"},\n",
    "    description=\"Detectron2 resnet 50 pre-trained model converted to torchscript\",\n",
    "    workspace=ws,\n",
    "    model_framework=Model.Framework.MULTI,\n",
    "    model_framework_version='21.02-py3', \n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy webservice\n",
    "\n",
    "Deploy to a pre-created [AksCompute](https://docs.microsoft.com/python/api/azureml-core/azureml.core.compute.aks.akscompute?view=azure-ml-py#provisioning-configuration-agent-count-none--vm-size-none--ssl-cname-none--ssl-cert-pem-file-none--ssl-key-pem-file-none--location-none--vnet-resourcegroup-name-none--vnet-name-none--subnet-name-none--service-cidr-none--dns-service-ip-none--docker-bridge-cidr-none--cluster-purpose-none--load-balancer-type-none-) named `aks-gpu`. For other options, see [our documentation](https://docs.microsoft.com/azure/machine-learning/how-to-deploy-and-where?tabs=azcli).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AksWebservice\n",
    "from azureml.core.model import InferenceConfig\n",
    "from random import randint\n",
    "\n",
    "service_name = \"detr-ncd-aks-gpu\"\n",
    "\n",
    "config = AksWebservice.deploy_configuration(\n",
    "    compute_target_name=\"aks-gpu\",\n",
    "    gpu_cores=1,\n",
    "    cpu_cores=1,\n",
    "    memory_gb=8,\n",
    "    auth_enabled=True,\n",
    ")\n",
    "\n",
    "service = Model.deploy(\n",
    "    workspace=ws,\n",
    "    name=service_name,\n",
    "    models=[model],\n",
    "    deployment_config=config,\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "service.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(service.get_logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the webservice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tritonclient.http as tritonhttpclient\n",
    "\n",
    "service_key = service.get_keys()[0]\n",
    "scoring_uri = service.scoring_uri[7:]\n",
    "headers = {}\n",
    "headers[\"Authorization\"] = f\"Bearer {service_key}\"\n",
    "\n",
    "triton_client = tritonhttpclient.InferenceServerClient(scoring_uri)\n",
    "\n",
    "model_name = \"resnet18\"\n",
    "\n",
    "# Check the state of server.\n",
    "health_ctx = triton_client.is_server_ready(headers=headers)\n",
    "print(\"Is server ready - {}\".format(health_ctx))\n",
    "\n",
    "# Check the status of model.\n",
    "status_ctx = triton_client.is_model_ready(model_name, \"1\", headers)\n",
    "print(\"Is model ready - {}\".format(status_ctx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tritonclient.utils import triton_to_np_dtype\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model_metadata = triton_client.get_model_metadata(model_name=model_name, headers=headers)\n",
    "\n",
    "input_meta = model_metadata[\"inputs\"]\n",
    "output_meta = model_metadata[\"outputs\"]\n",
    "\n",
    "np_dtype = triton_to_np_dtype(input_meta[0][\"datatype\"])\n",
    "\n",
    "dummy_input = torch.rand(1, 3, 224, 224)     # We should run a quick test\n",
    "input_ids = np.array(dummy_input[0], dtype=np_dtype)[None,...] # make bs=1\n",
    "\n",
    "inputs = []\n",
    "outputs = []\n",
    "        \n",
    "# Populate the inputs array\n",
    "input = tritonhttpclient.InferInput(input_meta[0][\"name\"], input_ids.shape, input_meta[0][\"datatype\"])\n",
    "input.set_data_from_numpy(input_ids, binary_data=False)\n",
    "inputs.append(input)\n",
    "\n",
    "# Populate the outputs array\n",
    "for out_meta in output_meta:\n",
    "    output_name = out_meta[\"name\"]\n",
    "    output = tritonhttpclient.InferRequestedOutput(output_name, binary_data=False)\n",
    "    outputs.append(output)\n",
    "\n",
    "            \n",
    "# Run inference\n",
    "res = triton_client.infer(\n",
    "    model_name,\n",
    "    inputs,\n",
    "    request_id=\"0\",\n",
    "    outputs=outputs,\n",
    "    model_version=\"1\",\n",
    "    headers=headers,\n",
    ")\n",
    "    \n",
    "out_data = res.as_numpy('output__0')\n",
    "tensor_arr = torch.from_numpy(out_data)\n",
    "\n",
    "hosted_top5 = F.softmax(tensor_arr, dim=1).topk(5).indices\n",
    "\n",
    "print('TorchScript model top 5 results:\\n  {}'.format(hosted_top5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the webservice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "service.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "\n",
    "Try reading [our documentation](https://aka.ms/triton-aml-docs) to use Triton with your own models or check out the other notebooks in this folder for ways to do pre- and post-processing on the server. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "name": "deploy-bidaf-aks",
  "task": "Use the high-performance Triton Inference Server with Azure Machine Learning"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
